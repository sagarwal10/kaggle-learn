{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS67_FINAL_PROJECT",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagarwal10/kaggle-learn/blob/master/CS67_FINAL_PROJECT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzI-hdK9Vtlk"
      },
      "source": [
        "# **CS67 Final Project  - Shalabh and Alex**\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "## Abstract:\r\n",
        "The world is filled with information and the rate at which it's produced only continues to grow. In this environment, a key challenge is learning to reliably distinguish between signal and noise to enhance decision making and understand identify trends. Companies, brands, and entertainers make product and service decisions on the basis of customer/audience feedback, but how does one do so effectively when the most direct channel for feedback - social media - is increasingly dominated by spam? To address this growing challenge, we will deploy several methods to identify spam using a public set of comments collected for spam research and determine which method most reliably identifies YouTube spam and leaves us with a data set of true commentary that can be used to conduct sentiment analysis from real fans and detractors. \r\n",
        "\r\n",
        "##Outline:\r\n",
        "\r\n",
        "**1. Introduction**:\r\n",
        "\r\n",
        "We are seeking to distinguish spam from genuine comments left on the videos of prominent entertainers. The source we are using, provided in Kaggle, includes five datasets composed by 1,956 real messages extracted from five videos that were among the 10 most viewed on the collection period. We seek to effectively identify spam among the comments.  \r\n",
        "\r\n",
        "**2. Exploratory data analysis:**\r\n",
        "\r\n",
        "Challenges we anticipate:\r\n",
        "-Abbreviations, capitalization, incorrect spelling, slang, and poor grammar \r\n",
        "-Habits of spam posters that may unknowingly be pre-processed out of the set (e.g., URLs in comments)  \r\n",
        "\r\n",
        "**3. Baseline Model**\r\n",
        "\r\n",
        "The methodology we plan to employ is outlined below. We will first attempt to run the methodology without pre-processing, to establish a baseline of results to compare against.   \r\n",
        "Using the text classification methodology outlined in Week 5 of the NLP course, we intend to examine the kaggle YouTube comments data set in order to identify and quantify spam and potentially to use the remaining non-spam comments to conduct a sentiment analysis. Methods include: \r\n",
        "*   Utilizing bag of words followed by TF-IDF as a subsequent model improvement to ultimately compare 4 different classification models (LinearSVC, LogisticRegression, RandomForestClassifier, and MultinomialNB)\r\n",
        "*   Time permitting, we would also like to utilize Latent Sentiment Analysis to see if we could find a vector for spam and  a vector that differentiates the 5 different artists\r\n",
        "\r\n",
        "**4. Model Improvement**\r\n",
        "\r\n",
        "Following the creation of the baseline to compare against, we will then pre-process the dataset using techniques including lemmatization, tokenization, stop-word removal, unusual character removal, URL tokenization replacement, lowercasing, expand contractions, white space removal. Following the pre-processing we will again deploy the 4 classification models and examine the improvement.\r\n",
        "\r\n",
        "The models will be broken out into training and test sets (80-20 distribution). Using K-Fold cross-validation, we will assess the accuracy of our models and compare which one or multiple ones are most effective for this data set.\r\n",
        "\r\n",
        "**5. Final results**: \r\n",
        "\r\n",
        "We will provide a presentation featuring a  thorough discussion of final quantitative metrics and qualitative visualizations. We will also discuss some of the biggest intermediate advancements that helped us improve the final model and share any insights we have derived from our results. \r\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SFC4jnbPnLz"
      },
      "source": [
        "### EDA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fdupj2sPrFh"
      },
      "source": [
        "The dataset contains of youtube comments from 5 different artists. We will collect them into a single Pandas Dataframe and examine the contents.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cbke-YOMP8tv",
        "outputId": "f6c9ad88-612d-4731-f2f8-5b3f666b8143"
      },
      "source": [
        "import matplotlib.pyplot as plt # plotting\n",
        "import numpy as np # linear algebra\n",
        "import os # accessing directory structure\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import collections\n",
        "\n",
        "dfYTSpam=pd.read_csv(\"https://sagarwal10-test.s3-us-west-2.amazonaws.com/kaggle_youtube_spam_dataset.csv\", delimiter=\",\")\n",
        "\n",
        "# since we were appending multiple CSV's we want to reindex the Data\n",
        "# https://stackoverflow.com/questions/35528119/pandas-recalculate-index-after-a-concatenation\n",
        "# dfYTSpam.reset_index(drop=True, inplace=True)\n",
        "nRow, nCol = dfYTSpam.shape\n",
        "print(f'There are {nRow} rows and {nCol} columns')\n",
        "print(dfYTSpam.iloc[1586]['content'])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1956 rows and 5 columns\n",
            "Nice song﻿\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubplvyg8RWyh"
      },
      "source": [
        "There are 1956 entries of data with 5 columns. For our purposes the two most meaningful columns are content and class. The content denotes the actual textual comment made by the user. The class is a label - TRUE if the comment is SPAM and FALSE if its a valid comment (i.e. ham). In our example above we see that entry 7 is a valid comment whereas the others are simply people pointing to other websites and thus spam.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pSovN6DSjDL"
      },
      "source": [
        "Amount of SPAM vs HAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1XX1qQuS0dO",
        "outputId": "ad54dffb-5238-4d42-efab-3ca391882d98"
      },
      "source": [
        "\n",
        "spam_class=dfYTSpam['class'] == True\n",
        "ham_class=dfYTSpam['class'] == False\n",
        "print(\" Number of SPAM comments: \" + str(len(dfYTSpam[spam_class])))\n",
        "print(\" Number of HAM comments: \" + str(len(dfYTSpam[ham_class])))\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Number of SPAM comments: 1005\n",
            " Number of HAM comments: 951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdUBTatuZqls"
      },
      "source": [
        "We see that the dataset is fairly evenly divided at 51.40% Spam. This means we should have decent training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oda7udvXZ-pw"
      },
      "source": [
        "Now lets see if we should considers authors as part of our dataset. This will only matter if certain authors are more likely to be habitual spammers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXLXOszxXz4t",
        "outputId": "e9cd118c-61e3-4e15-9e8f-c64f1c06d8b0"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "Counter(dfYTSpam[spam_class].duplicated(subset='author'))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({False: 871, True: 134})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UftsCRnlaYPT"
      },
      "source": [
        "This tells us that roughly 14% of the spam posts have been authored at least twice by the same person. We might consider using this in our model as an improvement. In practise a social media company would most likely use this. \n",
        "Next we will try to consider how many of the spam emails have a URL of a website included in them.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsaP5l0tZY19",
        "outputId": "d9186d7e-1d40-467b-9973-cf7f7e585e47"
      },
      "source": [
        "import re\n",
        "# Regular expression from https://www.geeksforgeeks.org/python-check-url-string/\n",
        "regex_url = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
        "print(\"Number of SPAM comments containing URLs = \" + str(len(dfYTSpam[spam_class][dfYTSpam[spam_class]['content'].str.contains(regex_url, regex=True)])))\n",
        "print(\"Number of HAM comments containing URLS = \" + str(len(dfYTSpam[ham_class][dfYTSpam[ham_class]['content'].str.contains(regex_url, regex=True)])))\n",
        "\n",
        "\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of SPAM comments containing URLs = 201\n",
            "Number of HAM comments containing URLS = 11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/strings.py:2001: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
            "  return func(self, *args, **kwargs)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "830zDCYwDHao",
        "outputId": "bc0e5151-524c-4466-fcf4-f1fffc252585"
      },
      "source": [
        "regex_url = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
        "dfYTSpam[spam_class][dfYTSpam[spam_class]['content'].str.contains(regex_url, regex=True)]\n",
        "# dfYTSpam.iloc[1586]['content']\n",
        "re.sub(regex_url, \"WebURL\", dfYTSpam.iloc[12]['content'], flags=re.MULTILINE)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/strings.py:2001: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
            "  return func(self, *args, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'WebURL'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnZyCGtxqyBp"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0G4i7aCrKmh"
      },
      "source": [
        "We see that 20% of SPAM contains URL vs only a percent in non spam class. So this might be a good optimization to remove URLs and replace them with a word like \"URL\" since its unlikely that the tokenizer will be able to produce anything useful token-wis from a URL.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2mKLaILM-HO"
      },
      "source": [
        "### Initial Model Train & Test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrtLfrb5OUHX"
      },
      "source": [
        "We are going to split the data into training and test data by a ration of 80-20 percent.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Y4U0A3aNMBv"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "trainComments, testComments, trainLabel, testLabel = train_test_split( dfYTSpam['content'], dfYTSpam['class'], test_size=.2, random_state=0)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5Hb9iNeOr49",
        "outputId": "63d1a916-5a9a-4e73-96f2-d140b18530c5"
      },
      "source": [
        "print(\"Number of samples of Training Data: \"+ str(len(trainComments)))\n",
        "print(\"Number of samples of Test Data: \" + str(len(testComments)))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples of Training Data: 1564\n",
            "Number of samples of Test Data: 392\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER-yklN6nbXy"
      },
      "source": [
        "We see that indeed the split is 80-20. Next we create a bag of words model using the CountVectorizer. We don't specify the stopwords parameter as we want to get an idea of the performance without doing any preprocessing.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YO1PiWin7tq",
        "outputId": "98eb7ab4-b6d5-4a69-afef-7815d208cc40"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "CntVec = CountVectorizer(binary=False, min_df=0.0, max_df=1.0)   \n",
        "trainingVec = CntVec.fit_transform(trainComments)     \n",
        "testingVec = CntVec.transform(testComments)   \n",
        "testingVec.shape\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(392, 3812)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3rq7LPA37vH"
      },
      "source": [
        "We see that we have transformed the data into a bag of words of dictionary size 3812."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRaVVJ6A6msr",
        "outputId": "7f750ceb-c0d2-4fd4-d97a-737bbe97cc6f"
      },
      "source": [
        "print(CntVec.get_feature_names()[0:10])\n",
        "print(CntVec.get_feature_names()[3800:3811])\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['00', '000', '002', '018', '02', '034', '04', '047000', '05', '053012']\n",
            "['zonepa', 'zxlightsoutxz', 'οh', 'الإعلانات', 'لل', '강남스타일', 'ｃｏｍ', 'ｅｂａｙ', 'ｈｔｔｐ', 'ｓｈｏｅｃｏｌｌｅｃｔｏｒ314', 'ｕｓｒ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-RDZpYpWajk"
      },
      "source": [
        "We notice that as a result of not preprocessing we have numbers and foreign characters that should definitely be removed. Next lets look at the distribution of length of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "OZ-zMjotMfUs",
        "outputId": "4c31bdc2-77d4-454d-8184-f0cee11bc5db"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        " \n",
        "cnt=Counter()\n",
        "for word in CntVec.get_feature_names():\n",
        "  cnt[len(word)] += 1\n",
        "fig = plt.figure(figsize = (10, 5))\n",
        "plt.bar(cnt.keys(), cnt.values(), width = 1)\n",
        "plt.xlim(0, 51)\n",
        "plt.xlabel('Length of token')\n",
        "plt.ylabel('Number of tokens')\n",
        "plt.show()\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8UsCzBuSDhh",
        "outputId": "88f7fdcc-9e30-48bc-eece-2cd4d9e032b5"
      },
      "source": [
        "for word in CntVec.get_feature_names():\n",
        "  if len(word) > 30:\n",
        "    print(word)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1fhenqx1twqm153v2ptayiejnealahzvem\n",
            "2flist_of_most_viewed_youtube_videos\n",
            "5bgkg2iwphzohwaeuesrwnegqg_labco7rw9wfx8hao\n",
            "89iyec7nrwp5nytno5u7amhvmflutggl\n",
            "90000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
            "dislikesssssssssssssssssssssssssssssssss\n",
            "i9tixe8cnkcosoqkemgjlu5uwv6nsxso\n",
            "lneadw26bfst76vhkjl8pxaey6vmnlvmriudtsfk6vy\n",
            "lneadw26bftvzqt6juehasiefrjg1exi_dvqdnqvpho\n",
            "lneadw26bfunoarag71awgu6tjo6azdkfiun_tz1_hy\n",
            "lneadw26bfvkahxpkenm25fywkyxthsupri6juqsznu\n",
            "plzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
            "soooooooooooooooooooooooooooooooooooooooo\n",
            "soooooooooooooooooooooooooooooooooooooooooooooooooo\n",
            "wowwwwwwwwwwwwwwwwwwwwwwwwwwwwww\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "UNBQs_R8o57h",
        "outputId": "6c7b35c8-abb9-455c-f0da-3ba9e301938a"
      },
      "source": [
        "filter=dfYTSpam['content'].str.find('soooooooooooooooooooooooooooooooooooooooo') > 0\n",
        "dfYTSpam[filter]"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_id</th>\n",
              "      <th>author</th>\n",
              "      <th>date</th>\n",
              "      <th>content</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>695</th>\n",
              "      <td>z12sjp3zgtqnvlysj23zuxxaolrvd1oj504</td>\n",
              "      <td>Kacy Cluley</td>\n",
              "      <td>2015-06-05T18:59:52</td>\n",
              "      <td>This song means so much to me thank you  soooo...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>807</th>\n",
              "      <td>z120t12qtr23etulr23rvzbyfyj1yfons</td>\n",
              "      <td>Mayen Dabuol</td>\n",
              "      <td>2015-05-22T01:59:18.387</td>\n",
              "      <td>Love this sooooooooooooooooooooooooooooooooooo...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              comment_id  ...  class\n",
              "695  z12sjp3zgtqnvlysj23zuxxaolrvd1oj504  ...  False\n",
              "807    z120t12qtr23etulr23rvzbyfyj1yfons  ...  False\n",
              "\n",
              "[2 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8--ARmuXF0T"
      },
      "source": [
        "We see from the bar graph that we have some really large words that are nonsensical and also a lot of repeated characters for regular English words to provide emphasis. Preprocessing can be used here as well.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pciE_oJBvyuO"
      },
      "source": [
        "We will start off by using a Bayes model to train and then come up with train and test accuracy scores\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seC5-Y08vl67",
        "outputId": "e5e4f921-799b-4132-d4a3-098dd7ce62a1"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "model=MultinomialNB()\n",
        "model.fit(trainingVec, trainLabel)\n",
        "arrCVS = sklearn.model_selection.cross_val_score(model, trainingVec, trainLabel, cv=5);\n",
        "\n",
        "print(\"Training data score/accuracy is \"+str(round(model.score(trainingVec, trainLabel), 4)))\n",
        "print(\"Testing data score/accuracy is \"+str(round(model.score(testingVec, testLabel), 4)))\n",
        "print(\"5 fold cross validation accuracy is \"+str(round(np.mean(arrCVS), 4)))\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data score/accuracy is 0.9642\n",
            "Testing data score/accuracy is 0.9158\n",
            "5 fold cross validation accuracy is 0.9188\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdsngGOZzsHX"
      },
      "source": [
        "We notice that even without preprocessing we get a fairly good accuracy of 92% with both the testing and the 5 fold cross validation test. Lets see if we can improve that with pre processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF2FOkSGavBI"
      },
      "source": [
        "### Preprocessing and retrain model \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN4DnrOZa5HP"
      },
      "source": [
        "We will now attempt to preprocess the comments to remove special characters, abbreviations, expand contractions, remove stopwords and of course repetitions of characters and try to see how that affects our NB model and its accuracy. All preprocessing code here comes from CS67 Week 5 colab with modifications.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1J9JHIobSmX",
        "outputId": "d1327466-120f-4b13-d9d8-7ddd5b99149c"
      },
      "source": [
        "#########################################################\n",
        "# Source: Text Analytics with Python by Dipanjan Sarkar\n",
        "#########################################################\n",
        "!pip -q install contractions > tmp.log\n",
        "import nltk, spacy, unicodedata, re, collections\n",
        "from contractions import contractions_dict\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from bs4 import BeautifulSoup\n",
        "#from textblob import Word\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download(['stopwords'], quiet=True)\n",
        "LosStopWords = nltk.corpus.stopwords.words('english')\n",
        "nlp = spacy.load('en_core_web_sm', parse=True, tag=True, entity=True)\n",
        "tokenizer = ToktokTokenizer()\n",
        "# nlp_vec = spacy.load('en_vectors_web_lg', parse=True, tag=True, entity=True)\n",
        "\n",
        "def RemHTMLTags(txt):\n",
        "    soup = BeautifulSoup(txt, \"html.parser\")\n",
        "    if bool(soup.find()): \n",
        "      # [s.extract() for s in soup(['iframe', 'script'])]\n",
        "      return re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', soup.get_text())\n",
        "    return txt\n",
        "\n",
        "def remove_repeated_characters(tokens):\n",
        "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
        "    match_substitution = r'\\1\\2\\3'\n",
        "    def replace(old_word):\n",
        "        if wordnet.synsets(old_word):\n",
        "            return old_word\n",
        "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
        "        return replace(new_word) if new_word != old_word else new_word\n",
        "            \n",
        "    correct_tokens = [replace(word) for word in tokens]\n",
        "    return correct_tokens\n",
        "\n",
        "def RemDupChar(tokens):\n",
        "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
        "    def replace(old_word):\n",
        "        if wordnet.synsets(old_word): return old_word\n",
        "        new_word = repeat_pattern.sub(r'\\1\\2\\3', old_word)\n",
        "        return replace(new_word) if new_word != old_word else new_word\n",
        "    return ''.join([replace(word) for word in tokens])\n",
        "\n",
        "def expand_contractions(text, contraction_mapping=contractions_dict):\n",
        "    \n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
        "                                      flags=re.IGNORECASE|re.DOTALL)\n",
        "    def expand_match(contraction):\n",
        "        match = contraction.group(0)\n",
        "        first_char = match[0]\n",
        "        if first_char == None:\n",
        "          print(contraction)\n",
        "        expanded_contraction = contraction_mapping.get(match)\\\n",
        "                                if contraction_mapping.get(match)\\\n",
        "                                else contraction_mapping.get(match.lower())\n",
        "        if expanded_contraction == None:\n",
        "          # print(match)\n",
        "          # print(contraction_mapping.get(match))\n",
        "          # print(contraction_mapping.get(match.lower()))\n",
        "          # print(\"Expanded is false\")           \n",
        "          expanded_contraction=match           \n",
        "        expanded_contraction = first_char+expanded_contraction[1:]\n",
        "        return expanded_contraction\n",
        "        \n",
        "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "    return expanded_text\n",
        "def ExpContractions(txt, cmap=contractions_dict):\n",
        "    cpat = re.compile('({})'.format('|'.join(cmap.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
        "    def expand_match(contraction):\n",
        "        return contraction.group(0)[0]+(cmap.get(match) if cmap.get(match) else cmap.get(match.lower()))[1:]\n",
        "    return re.sub(\"'\", \"\", cpat.sub(expand_match, txt))\n",
        "\n",
        "def Stem(txt): return ' '.join([nltk.porter.PorterStemmer().stem(word) for word in txt.split()])\n",
        "def lemmatize_text(txt): return ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in nlp(txt)])\n",
        "def RemAccChar(txt): return unicodedata.normalize('NFKD', txt).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "def RemSpecChar(txt, remove_digits=False): return re.sub(r'[^a-zA-Z0-9\\s]|\\[|\\]' if not remove_digits else r'[^a-zA-Z\\s]|\\[|\\]', '', txt)\n",
        "def ReplaceURL(txt):      #Replace actual URLs with the string likelyspamurl\n",
        "    return(re.sub(regex_url, \"likelyspamurl\", txt, flags=re.MULTILINE))\n",
        "def RemStopwords(txt, lower=False, stopwords=LosStopWords):\n",
        "    tokens = [token.strip() for token in tokenizer.tokenize(txt)]\n",
        "    return ' '.join([t for t in tokens if (t if lower else t.lower()) not in stopwords])\n",
        "\n",
        "def normalize_corpus(corpus, repURL=False, remDup=False, remHTML=False, expContr=False, remAccChar=False, lower=False, stem=False, \n",
        "                     lemma=False, remSpecChar=False, remDig=False, remStop=False, stopwords=LosStopWords):\n",
        "    normalized_corpus = []\n",
        "    # just to keep negation if any in bi-grams\n",
        "    if 'no' in stopwords: stopwords.remove('no')\n",
        "    if 'not' in stopwords: stopwords.remove('not')\n",
        "    for doc in corpus:   # normalize each document in the corpus\n",
        "        if remHTML: doc = RemHTMLTags(doc) # strip HTML\n",
        "        doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \")) # remove extra newlines\n",
        "        if repURL: doc=ReplaceURL(doc)\n",
        "        if remAccChar: doc = RemAccChar(doc) # remove accented characters\n",
        "        if expContr: doc = expand_contractions(doc) # expand contractions\n",
        "        if remDup: \n",
        "            correct_tokens = remove_repeated_characters(nltk.word_tokenize(doc))\n",
        "            # ' '.join(correct_tokens)\n",
        "            # correct_tokens = RemDupChar(nltk.word_tokenize(doc))\n",
        "            doc=' '.join(correct_tokens)\n",
        "        if lemma: doc = lemmatize_text(doc)  # lemmatize text        \n",
        "        if stem and not lemma: doc = Stem(doc) # stem text\n",
        "        \n",
        "        if remSpecChar: # remove special characters and\\or digits    \n",
        "            # insert spaces between special characters to isolate them    \n",
        "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "            doc = RemSpecChar(doc, remove_digits=remDig)  \n",
        "\n",
        "        doc = re.sub(' +', ' ', doc) # remove extra whitespace\n",
        "        if lower: doc = doc.lower() # lowercase the text    \n",
        "        # remove stopwords\n",
        "        if remStop: doc = RemStopwords(doc, lower=lower, stopwords=LosStopWords)\n",
        "        doc = re.sub(' +', ' ', doc).strip() # remove extra whitespace\n",
        "        normalized_corpus.append(doc)\n",
        "    return normalized_corpus\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzay3Q4xo2kM"
      },
      "source": [
        "def preprocess_corpus_train_model_and_score(cleaned_content_colname, **kwargs):\n",
        "   dfYTSpam[cleaned_content_colname] = normalize_corpus(corpus=dfYTSpam.content, **kwargs)\n",
        "   trainComments, testComments, trainLabel, testLabel = train_test_split( dfYTSpam[cleaned_content_colname], dfYTSpam['class'], test_size=.2, random_state=0)\n",
        "   CntVec = CountVectorizer(binary=False, min_df=0.0, max_df=1.0)   \n",
        "   trainingVec = CntVec.fit_transform(trainComments)     \n",
        "   testingVec = CntVec.transform(testComments)   \n",
        "   testingVec.shape\n",
        "   model=MultinomialNB()\n",
        "   model.fit(trainingVec, trainLabel)\n",
        "   arrCVS = sklearn.model_selection.cross_val_score(model, trainingVec, trainLabel, cv=5)\n",
        "   print(\"Accuracies - Training: \"+str(round(model.score(trainingVec, trainLabel), 4))  \\\n",
        "         + \"\\tTesting: \"+str(round(model.score(testingVec, testLabel),4)) + \\\n",
        "         \"\\t\\t5-fold KCV: \"+str(round(np.mean(arrCVS), 4)) + \\\n",
        "         \"\\tNum features: \"+str(len(CntVec.get_feature_names())))"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdGgpa35IvB1"
      },
      "source": [
        "The code above will preprocess the comments, train a NB model and then compute accuracy scores. It takes in keywords for different preprocessing options.\n",
        "We first call this new function with no options for preprocessing and we see that indeed the scores match exactly the numbers we got earlier also with no preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98sEgYLqrikC",
        "outputId": "f2189ed1-702f-4703-b103-22cc448688fe"
      },
      "source": [
        "preprocess_corpus_train_model_and_score('newcontent')"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracies - Training: 0.9642\tTesting: 0.9158\t\t5-fold KCV: 0.9188\tNum features: 3812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV5RN0znQK2D"
      },
      "source": [
        "Next we will try our hypothesis to replace actual URLS in the comments with a word token (in this case 'likelyspamurl') to see if that helps the model since its assumed actual url's are unlikely to be of use\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIQpGvk6tx3G",
        "outputId": "9c6b3065-ed87-4058-f976-d6e04728f3a6"
      },
      "source": [
        "preprocess_corpus_train_model_and_score('newcontent', repURL=True)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracies - Training: 0.9591\tTesting: 0.9031\t\t5-fold KCV: 0.9124\tNum features: 3319\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeb5hwfzRG9I"
      },
      "source": [
        "We notice that replacing actual URLs with a single known keyword reduced the number of CountVectizer feature names from 3812 to 3319 - a reduction of 13%. However we see that our scores SURPRISINGLY went down. Its possible that the different URLs help our model detect spam better so we will no longer replace URLs.\n",
        "Next we will  test regular preprocessing features one by one adding them and seeing if the scores improve until we find the most optimal combination.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EUUZ5Z20Diw",
        "outputId": "3b56921c-d7c4-410f-f12e-93bc497dcd83"
      },
      "source": [
        "preprocess_corpus_train_model_and_score('newcontent', remDup=True)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracies - Training: 0.9636\tTesting: 0.9184\t\t5-fold KCV: 0.9207\tNum features: 3769\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUdVqEVL0YTa",
        "outputId": "41eb5c2d-1660-4731-f0ef-997853bc3071"
      },
      "source": [
        "preprocess_corpus_train_model_and_score('newcontent', remDup=True, expContr=True)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracies - Training: 0.9642\tTesting: 0.9184\t\t5-fold KCV: 0.9201\tNum features: 3766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfdCK6Gz0jdp",
        "outputId": "c5ccb6b4-aa4a-4424-dff6-5699cca73aec"
      },
      "source": [
        "preprocess_corpus_train_model_and_score('newcontent', remDup=True, expContr=True, remAccChar=True)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracies - Training: 0.9655\tTesting: 0.926\t\t5-fold KCV: 0.9214\tNum features: 3743\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSZ10_3t0ukw",
        "outputId": "35afdc91-88f7-4085-9d2d-744c55592eb8"
      },
      "source": [
        "preprocess_corpus_train_model_and_score('newcontent', remDup=True, expContr=True, remAccChar=True, lower=True, stem=True)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracies - Training: 0.9591\tTesting: 0.9286\t\t5-fold KCV: 0.922\tNum features: 3335\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhTei4BM2cmq",
        "outputId": "2f38653d-622d-49e2-f48a-49f491c4eb29"
      },
      "source": [
        "preprocess_corpus_train_model_and_score('newcontent', repURL=True, remDup=True, expContr=True, remAccChar=True, lower=True, stem=True)"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracies - Training: 0.9591\tTesting: 0.9209\t\t5-fold KCV: 0.9182\tNum features: 2827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ro_3BMH-1AOa",
        "outputId": "46aced8d-be72-486e-9ff9-75c2a7ee97eb"
      },
      "source": [
        "preprocess_corpus_train_model_and_score('newcontent', remDup=True, expContr=True, remAccChar=True, lower=True, stem=True, remSpecChar=True)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracies - Training: 0.9578\tTesting: 0.9235\t\t5-fold KCV: 0.9214\tNum features: 3202\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbvJaxwU1zHJ",
        "outputId": "ef921bb4-ba22-4f82-a788-1e4aca7ded48"
      },
      "source": [
        "preprocess_corpus_train_model_and_score('newcontent', remDup=True, expContr=True, remAccChar=True, lower=True, stem=True, remSpecChar=True, remDig=True)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracies - Training: 0.954\tTesting: 0.9184\t\t5-fold KCV: 0.9194\tNum features: 2977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8_TiVt_2AKE",
        "outputId": "5fdb12f4-600a-4e1f-f26e-5890da7f6b4b"
      },
      "source": [
        "preprocess_corpus_train_model_and_score('newcontent', remDup=True, expContr=True, remAccChar=True, lower=True, stem=True, remSpecChar=True, remDig=True, remStop=True)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracies - Training: 0.9578\tTesting: 0.9031\t\t5-fold KCV: 0.9041\tNum features: 2875\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}